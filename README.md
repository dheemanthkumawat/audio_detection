# Live Audio Analysis Pipeline

Real-time audio processing pipeline using PANNs CNN14 for audio classification, Vosk for speech recognition, and intelligent anomaly detection with speech session management.

## Requirements

- **Python 3.9+** (tested on 3.9, 3.10, 3.11)
- **CUDA-capable GPU** (optional, for faster PANNs inference)
- **Microphone** with audio input capabilities

## Quick Start

1. **Create virtual environment:**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Download Vosk model:**
   ```bash
   mkdir -p models
   wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip -O models/vosk-model.zip
   cd models && unzip vosk-model.zip
   mv vosk-model-small-en-us-0.15 vosk-small-en
   rm vosk-model.zip
   cd ..
   ```

4. **List audio devices (find your microphone):**
   ```bash
   python main.py --list-devices
   ```

5. **Configure audio device (optional):**
   ```bash
   cp .env.example .env
   # Edit .env and set AUDIO_DEVICE_INDEX to your microphone index
   ```

6. **Run with default config:**
   ```bash
   python main.py
   ```

7. **Run with quiet-office profile:**
   ```bash
   python main.py --profile quiet-office
   ```

## Features

- **ğŸ¯ Real-time audio classification** using PANNs CNN14 (527 AudioSet classes)
- **ğŸ§  Intelligent speech session management** - prevents bouncing, captures complete conversations
- **ğŸ”„ Hybrid transcription approach** - multiple triggers for reliable speech capture
- **ğŸ“Š 5-second rolling buffer** - ensures no speech is lost during processing
- **âš ï¸ Anomaly detection** for sirens, alarms, glass breaking, screams, gunshots
- **ğŸ—£ï¸ Offline speech-to-text** using Vosk (no internet required)
- **ğŸ˜Š Sentiment analysis** with configurable keyword matching
- **ğŸ’¾ Local data storage** - CSV files for analysis, JSON for events
- **ğŸ“¡ MQTT publishing** for real-time event streaming
- **âš™ï¸ Configurable profiles** - default and quiet-office presets

## Configuration

- Default config: `config/default.yaml`
- Profiles: `config/quiet-office.yaml`  
- Environment overrides: `.env` (see `.env.example`)

## Architecture

```
audio_detection/
â”œâ”€â”€ main.py                 # Entry point
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline.py         # Main pipeline orchestration
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”œâ”€â”€ processor.py    # Audio capture & buffering
â”‚   â”‚   â””â”€â”€ speech_session.py # Speech session management
â”‚   â”œâ”€â”€ classification/
â”‚   â”‚   â””â”€â”€ audio_classifier.py # PANNs + Vosk integration
â”‚   â”œâ”€â”€ events/
â”‚   â”‚   â”œâ”€â”€ logger.py       # MQTT/Event logging
â”‚   â”‚   â””â”€â”€ storage.py      # Local data storage
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ config.py       # Configuration management
â”œâ”€â”€ config/                 # YAML configuration files
â”œâ”€â”€ data/                   # Local storage (CSV + JSON)
â”œâ”€â”€ logs/                   # Application logs
â”œâ”€â”€ models/                 # Vosk STT models
â””â”€â”€ requirements.txt        # Python dependencies
```

## Output

Events are logged in real-time to console and stored locally:

**Console Output:**
```
ğŸ”´ Live Audio Pipeline Starting...
   ğŸ“Š PANNs CNN14 for audio classification
   ğŸ—£ï¸  Vosk for speech recognition
   ğŸ’¾ Local storage enabled
   â¸ï¸  Ctrl-C to quit

[PANNs] Speech (0.847) [Window 45]
ğŸ—£ Speech (session_active): hello world this is a complete sentence
ğŸš¨ 14:23:15 ABNORMAL ALARM (0.92)

ğŸ“Š Session Statistics:
   â€¢ Windows processed: 245
   â€¢ Speech sessions: 3  
   â€¢ Anomalies detected: 1
   â€¢ Today's events: 12
   â€¢ Speech transcripts: 8
```

**Local Storage:**
- `data/events.jsonl` - All events in JSON format
- `data/speech_transcripts.csv` - Speech transcriptions with metadata  
- `data/anomalies.csv` - Detected anomalies with classifications
- `data/daily_summary.json` - Daily statistics and summaries

## Troubleshooting

**Common Issues:**

1. **No audio device found:**
   ```bash
   python main.py --list-devices
   # Copy the correct device index to .env file
   ```

2. **Vosk model not found:**
   ```bash
   # Re-download and extract the model
   cd models && wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip
   ```

3. **MQTT connection failed:**
   ```bash
   # Install and start MQTT broker (optional)
   sudo apt install mosquitto mosquitto-clients
   sudo systemctl start mosquitto
   ```

4. **GPU not detected:**
   ```bash
   # Install CUDA-enabled PyTorch (optional, for faster inference)
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   ```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.